# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10BJGfGqHxabNK-Xrq9wW4Fc9UQBm_rfg
"""

"""
nssm_forecast.py

Single-file implementation:
- synthetic multivariate dataset generator (n_features >= 5)
- Neural State Space Model (NSSM) implemented in PyTorch
- Baseline LSTM model
- Training loops, validation, testing
- Forecasting (multi-step) and error metrics: RMSE, WAPE, MASE
- Quick sensitivity sweep across latent dimensions

Run:
    python nssm_forecast.py
"""

import math
import random
import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from typing import Tuple, Dict

# Reproducibility
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
random.seed(SEED)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# -------------------------
# Synthetic dataset utils
# -------------------------
def generate_multivariate_series(
    n_series: int = 1,
    length: int = 3000,
    n_features: int = 5,
    seasonal_periods: Tuple[int, ...] = (24, 168),
    trend_coeff: float = 0.001,
    noise_std: float = 0.1,
    structural_breaks: int = 2,
) -> np.ndarray:
    """
    Generate synthetic multivariate time series of shape (length, n_features)
    with multiple seasonalities, trend, noise, and occasional structural breaks.
    """
    t = np.arange(length)
    data = np.zeros((length, n_features), dtype=np.float32)

    # base seasonal patterns per feature (phase offsets)
    for f in range(n_features):
        phase = np.random.uniform(0, 2 * np.pi)
        amp = np.random.uniform(0.5, 2.0)
        seasonal = np.zeros(length)
        for p in seasonal_periods:
            seasonal += amp * np.sin(2 * np.pi * t / p + phase) / (len(seasonal_periods))
        trend = trend_coeff * t * (1 + np.random.uniform(-0.5, 0.5))
        noise = np.random.normal(0, noise_std, size=length)
        data[:, f] = seasonal + trend + noise

    # Add feature interactions (simple linear combos) and structural breaks
    for b in range(structural_breaks):
        pos = np.random.randint(length // 10, length - length // 10)
        magnitude = np.random.uniform(-1.0, 1.0)
        width = np.random.randint(10, 100)
        ramp = np.clip((t - pos + width) / width, 0, 1)
        for f in range(n_features):
            data[:, f] += magnitude * ramp * (0.5 + 0.5 * np.random.randn())

    # Introduce small cross-feature correlations
    if n_features >= 2:
        for i in range(1, n_features):
            data[:, i] += 0.1 * data[:, 0]  # small shared component

    return data


# -------------------------
# Dataset wrapper
# -------------------------
class TimeSeriesDataset(Dataset):
    def __init__(self, series: np.ndarray, input_len: int, pred_len: int):
        """
        series: (T, features)
        input_len: number of timesteps used to condition
        pred_len: forecast horizon
        """
        self.series = series.astype(np.float32)
        self.input_len = input_len
        self.pred_len = pred_len
        self.T = series.shape[0]
        self.n_features = series.shape[1]
        self.indices = [
            i for i in range(0, self.T - (input_len + pred_len) + 1)
        ]

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, idx):
        i = self.indices[idx]
        x = self.series[i : i + self.input_len]  # (input_len, n_features)
        y = self.series[i + self.input_len : i + self.input_len + self.pred_len]
        return x, y


# -------------------------
# Evaluation metrics
# -------------------------
def rmse(pred: np.ndarray, true: np.ndarray) -> float:
    return float(np.sqrt(np.mean((pred - true) ** 2)))


def wape(pred: np.ndarray, true: np.ndarray) -> float:
    denom = np.sum(np.abs(true))
    return float(np.sum(np.abs(pred - true)) / (denom + 1e-8))


def mase(pred: np.ndarray, true: np.ndarray, insample: np.ndarray, seasonality: int = 1) -> float:
    """
    Mean Absolute Scaled Error. For multi-step/multivariate, flatten arrays.
    insample: reference insample series used to compute naive forecast errors; shape (T_insample, features)
    seasonality: step for naive forecast (1 for one-step naive)
    """
    n = insample.shape[0]
    # naive one-step differences
    denom = np.mean(np.abs(insample[seasonality:] - insample[:-seasonality]))
    mae = np.mean(np.abs(pred - true))
    return float(mae / (denom + 1e-8))


# -------------------------
# Neural State Space Model
# -------------------------
class NSSM(nn.Module):
    def __init__(
        self,
        n_features: int,
        latent_dim: int = 16,
        transition_hidden: int = 64,
        obs_hidden: int = 64,
    ):
        """
        Discrete-time Neural State Space Model:
            z_{t+1} = z_t + f(z_t, u_t)
            y_t     = g(z_t)
        where f and g are small MLPs. We use residual / Euler structure to stabilize training.
        """
        super().__init__()
        self.latent_dim = latent_dim
        self.n_features = n_features

        # initial encoder: map recent window mean to initial latent state
        self.encoder = nn.Sequential(
            nn.Linear(n_features, latent_dim),
            nn.ReLU(),
            nn.Linear(latent_dim, latent_dim),
        )

        # transition function f: takes z_t and optionally y_t (here we use z_t only)
        self.transition = nn.Sequential(
            nn.Linear(latent_dim + n_features, transition_hidden),
            nn.ReLU(),
            nn.Linear(transition_hidden, latent_dim),
        )

        # observation function g: maps latent to observation
        self.observation = nn.Sequential(
            nn.Linear(latent_dim, obs_hidden),
            nn.ReLU(),
            nn.Linear(obs_hidden, n_features),
        )

    def forward(self, x_seq: torch.Tensor, horizon: int, teacher_forcing_y: torch.Tensor = None):
        """
        x_seq: (batch, input_len, n_features) - conditioning window
        horizon: forecast steps to generate
        teacher_forcing_y: optional (batch, horizon, n_features) to run closed-loop with teacher forcing (used for training)
        Returns:
            preds: (batch, horizon, n_features)
        """
        batch, input_len, n_features = x_seq.shape
        assert n_features == self.n_features

        # initialize latent z0 from last timestep of x_seq (or mean)
        x_last = x_seq[:, -1, :]  # (batch, n_features)
        z = self.encoder(x_last)  # (batch, latent_dim)

        preds = []
        # iterate for forecast horizon
        for t in range(horizon):
            # optionally include last available observation as input to transition
            obs_input = x_last if t == 0 else (teacher_forcing_y[:, t - 1] if teacher_forcing_y is not None else preds[-1])
            trans_input = torch.cat([z, obs_input], dim=-1)  # (batch, latent + features)
            dz = self.transition(trans_input)  # (batch, latent_dim)
            # residual update (Euler-like)
            z = z + dz
            y = self.observation(z)
            preds.append(y)
            # update x_last for next step
            x_last = y.detach()  # stop gradient if teacher forcing not used

        preds = torch.stack(preds, dim=1)
        return preds


# -------------------------
# Baseline LSTM
# -------------------------
class LSTMForecaster(nn.Module):
    def __init__(self, n_features: int, hidden: int = 64, n_layers: int = 1, pred_len: int = 24):
        super().__init__()
        self.n_features = n_features
        self.hidden = hidden
        self.n_layers = n_layers
        self.pred_len = pred_len

        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden, num_layers=n_layers, batch_first=True)
        self.proj = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, n_features))

    def forward(self, x_seq: torch.Tensor):
        """
        x_seq: (batch, input_len, features)
        We'll roll out autoregressively using the LSTM hidden state.
        """
        batch = x_seq.shape[0]
        out, (h, c) = self.lstm(x_seq)  # out: (batch, input_len, hidden)
        hx = (h, c)
        last_h = out[:, -1, :]  # (batch, hidden)
        # autoregressive forecasting in latent LSTM space
        preds = []
        input_t = x_seq[:, -1, :]  # last observed
        hidden = hx
        for _ in range(self.pred_len):
            input_t = input_t.unsqueeze(1)  # (batch,1,features)
            out_t, hidden = self.lstm(input_t, hidden)
            y = self.proj(out_t.squeeze(1))
            preds.append(y)
            input_t = y
        preds = torch.stack(preds, dim=1)
        return preds


# -------------------------
# Training helpers
# -------------------------
def train_epoch(model, loader, optimizer, criterion, clip_grad=None, teacher_forcing=False):
    model.train()
    total_loss = 0.0
    n_batches = 0
    for x, y in loader:
        x = x.to(DEVICE)
        y = y.to(DEVICE)
        optimizer.zero_grad()
        if isinstance(model, NSSM):
            # feed teacher_forcing_y only if requested
            tf_y = y if teacher_forcing else None
            preds = model(x, horizon=y.shape[1], teacher_forcing_y=tf_y)
        else:
            preds = model(x)
        loss = criterion(preds, y)
        loss.backward()
        if clip_grad:
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)
        optimizer.step()
        total_loss += float(loss.item())
        n_batches += 1
    return total_loss / max(1, n_batches)


def evaluate(model, loader, criterion):
    model.eval()
    total_loss = 0.0
    n_batches = 0
    preds_list = []
    true_list = []
    with torch.no_grad():
        for x, y in loader:
            x = x.to(DEVICE)
            y = y.to(DEVICE)
            if isinstance(model, NSSM):
                preds = model(x, horizon=y.shape[1], teacher_forcing_y=None)
            else:
                preds = model(x)
            loss = criterion(preds, y)
            total_loss += float(loss.item())
            n_batches += 1
            preds_list.append(preds.cpu().numpy())
            true_list.append(y.cpu().numpy())
    preds_np = np.concatenate(preds_list, axis=0)
    true_np = np.concatenate(true_list, axis=0)
    return total_loss / max(1, n_batches), preds_np, true_np


# -------------------------
# Utility: data scaling
# -------------------------
class StandardScaler:
    def __init__(self):
        self.mean = None
        self.std = None

    def fit(self, arr: np.ndarray):
        self.mean = arr.mean(axis=0)
        self.std = arr.std(axis=0)
        self.std[self.std == 0] = 1.0

    def transform(self, arr: np.ndarray) -> np.ndarray:
        return (arr - self.mean) / self.std

    def inverse(self, arr: np.ndarray) -> np.ndarray:
        return arr * self.std + self.mean


# -------------------------
# Main experiment
# -------------------------
def run_experiment(
    length=3000,
    n_features=5,
    input_len=96,
    pred_len=24,
    latent_dim=16,
    epochs=30,
    batch_size=64,
    lr=1e-3,
    early_stop=6,
):
    # 1) Generate data
    data = generate_multivariate_series(length=length, n_features=n_features)
    # split
    train_frac = 0.7
    val_frac = 0.15
    T = data.shape[0]
    t_train = int(T * train_frac)
    t_val = int(T * (train_frac + val_frac))
    train_series = data[:t_train]
    val_series = data[t_train - input_len : t_val]  # include buffer for windowing
    test_series = data[t_val - input_len :]

    scaler = StandardScaler()
    scaler.fit(train_series)
    train_scaled = scaler.transform(train_series)
    val_scaled = scaler.transform(val_series)
    test_scaled = scaler.transform(test_series)

    # datasets
    train_ds = TimeSeriesDataset(train_scaled, input_len=input_len, pred_len=pred_len)
    val_ds = TimeSeriesDataset(val_scaled, input_len=input_len, pred_len=pred_len)
    test_ds = TimeSeriesDataset(test_scaled, input_len=input_len, pred_len=pred_len)
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)

    # 2) Models
    nssm = NSSM(n_features=n_features, latent_dim=latent_dim).to(DEVICE)
    lstm = LSTMForecaster(n_features=n_features, hidden=latent_dim, pred_len=pred_len).to(DEVICE)

    # Optimizers & loss
    crit = nn.MSELoss()
    opt_n = torch.optim.Adam(nssm.parameters(), lr=lr)
    opt_l = torch.optim.Adam(lstm.parameters(), lr=lr)

    # 3) Train with early stopping (separately for both models)
    def fit_model(model, optimizer, name="model"):
        best_val = float("inf")
        best_state = None
        patience = 0
        for ep in range(epochs):
            tr_loss = train_epoch(model, train_loader, optimizer, crit, clip_grad=1.0, teacher_forcing=False)
            val_loss, _, _ = evaluate(model, val_loader, crit)
            # print status
            print(f"[{name}] Epoch {ep+1}/{epochs} | train_loss={tr_loss:.6f} val_loss={val_loss:.6f}")
            if val_loss < best_val - 1e-6:
                best_val = val_loss
                best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
                patience = 0
            else:
                patience += 1
                if patience >= early_stop:
                    print(f"[{name}] Early stopping at epoch {ep+1}")
                    break
        # load best
        if best_state is not None:
            model.load_state_dict(best_state)
        return model

    print("\nTraining NSSM...")
    nssm = fit_model(nssm, opt_n, name="NSSM")
    print("\nTraining LSTM baseline...")
    lstm = fit_model(lstm, opt_l, name="LSTM")

    # 4) Evaluate on test set
    _, preds_n, true_n = evaluate(nssm, test_loader, crit)
    _, preds_l, true_l = evaluate(lstm, test_loader, crit)

    # inverse scale
    preds_n = scaler.inverse(preds_n.reshape(-1, n_features)).reshape(preds_n.shape)
    preds_l = scaler.inverse(preds_l.reshape(-1, n_features)).reshape(preds_l.shape)
    true_np = scaler.inverse(true_n.reshape(-1, n_features)).reshape(true_n.shape)

    # compute metrics over flattened arrays (all horizons & features)
    rmse_n = rmse(preds_n, true_np)
    rmse_l = rmse(preds_l, true_np)
    wape_n = wape(preds_n, true_np)
    wape_l = wape(preds_l, true_np)

    # MASE uses training insample series as naive baseline
    insample = train_series  # original scale
    mase_n = mase(preds_n, true_np, insample, seasonality=1)
    mase_l = mase(preds_l, true_np, insample, seasonality=1)

    results = {
        "rmse": {"nssm": rmse_n, "lstm": rmse_l},
        "wape": {"nssm": wape_n, "lstm": wape_l},
        "mase": {"nssm": mase_n, "lstm": mase_l},
        "preds_nssm": preds_n,
        "preds_lstm": preds_l,
        "true": true_np,
    }
    return results


# -------------------------
# Quick sensitivity sweep
# -------------------------
if __name__ == "__main__":
    # Default configuration
    length = 3000
    n_features = 6
    input_len = 96
    pred_len = 24
    epochs = 30
    batch_size = 128
    lr = 1e-3

    latent_dims = [8, 16]  # short sweep; add more like [8,16,32] to test
    summary = {}

    for ld in latent_dims:
        print("\n" + "=" * 60)
        print(f"Running experiment with latent_dim = {ld}")
        res = run_experiment(
            length=length,
            n_features=n_features,
            input_len=input_len,
            pred_len=pred_len,
            latent_dim=ld,
            epochs=epochs,
            batch_size=batch_size,
            lr=lr,
            early_stop=6,
        )
        summary[ld] = {
            "rmse_nssm": res["rmse"]["nssm"],
            "rmse_lstm": res["rmse"]["lstm"],
            "wape_nssm": res["wape"]["nssm"],
            "wape_lstm": res["wape"]["lstm"],
            "mase_nssm": res["mase"]["nssm"],
            "mase_lstm": res["mase"]["lstm"],
        }
        print(f"Results latent_dim={ld} | NSSM RMSE={res['rmse']['nssm']:.4f} LSTM RMSE={res['rmse']['lstm']:.4f}")

    print("\nSummary sweep:")
    for k, v in summary.items():
        print(f"latent={k} -> {v}")

    # Example: save predictions for the last run to npz
    np.savez_compressed("nssm_preds.npz", preds_nssm=res["preds_nssm"], preds_lstm=res["preds_lstm"], true=res["true"])
    print("\nSaved preds to nssm_preds.npz")